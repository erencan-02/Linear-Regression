{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from IPython import display\n",
    "import pylab as pl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def generate_linear_regression_data(n=100, input_dim=1, output_dim=1, noise=20):\n",
    "    return datasets.make_regression(n_samples=n, n_features=input_dim, n_targets=output_dim, noise=noise)\n",
    "    \n",
    "    \n",
    "def linear_regression(X, y):\n",
    "    X = np.c_[np.ones(X.shape[0]), X]\n",
    "    beta = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "    return beta\n",
    "\n",
    "\n",
    "class LinearRegression():\n",
    "    def __init__(self, X, y, batch_size=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.num_samples = self.X.shape[0]\n",
    "        self.input_dim = 1 if len(self.X.shape) == 1 else self.X.shape[1]\n",
    "        self.output_dim = 1 if len(self.y.shape) == 1 else self.y.shape[1]\n",
    "        self.d = self.input_dim + 1\n",
    "        \n",
    "        # n x d => n data points\n",
    "        self.data = np.c_[np.ones(self.num_samples), self.X]\n",
    "        \n",
    "        self.betha = self.init_params()\n",
    "        self.actual_betha = self.analytical_solution(self.X, self.y)\n",
    "        self.loss_function = lambda y, y_hat: np.mean(np.square(y.reshape(self.output_dim, 1) - y_hat.reshape(self.output_dim, 1)))\n",
    "        self.loss_function_derivative = lambda y_i, y_i_hat, x_i: -2 * (y_i_hat - y_i).reshape(self.output_dim, 1) @ x_i.reshape(1, self.d)\n",
    "        self.is_batch = batch_size is not None\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def init_params(self):\n",
    "        return (np.random.rand(self.output_dim, self.d) - 0.5) * 100\n",
    "    \n",
    "    def analytical_solution(self, X, y):\n",
    "        X = np.c_[np.ones(X.shape[0]), X]\n",
    "        beta = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "        return beta.T  \n",
    "    \n",
    "    def step(self, lr=1e-3):\n",
    "        dE_db = np.zeros((self.output_dim, self.d))\n",
    "        n = self.num_samples\n",
    "        loss_sum = 0\n",
    "\n",
    "        if self.is_batch:\n",
    "            indices = np.random.permutation(self.num_samples)[:self.batch_size]\n",
    "            batch_data = self.data[indices]\n",
    "            batch_labels = self.y[indices]\n",
    "            n = self.batch_size\n",
    "        else:\n",
    "            batch_data = self.data\n",
    "            batch_labels = self.y\n",
    "                        \n",
    "        for j in range(n):\n",
    "            x_j = batch_data[j]\n",
    "            y_j = batch_labels[j]\n",
    "                \n",
    "            # Forward pass\n",
    "            outputs = self.betha @ x_j\n",
    "                \n",
    "            # # Error / Loss\n",
    "            loss = self.loss_function(outputs, y_j)\n",
    "            loss_sum += loss\n",
    "                \n",
    "            # Accumulate derivative\n",
    "            # Do some vector calculus shit\n",
    "            dE_db += self.loss_function_derivative(outputs, y_j, x_j)\n",
    "            \n",
    "        self.betha -= lr * dE_db / self.num_samples\n",
    "        \n",
    "        # Returns parameters and average loss over batch\n",
    "        return self.betha, loss_sum / n\n",
    "    \n",
    "    def calculate_total_loss(self, betha=None):\n",
    "        total_loss = 0\n",
    "        \n",
    "        betha = self.betha if betha is None else betha\n",
    "        \n",
    "        for i in range(self.data.shape[0]):\n",
    "            total_loss += self.loss_function(betha @ self.data[i], self.y[i])    \n",
    "        \n",
    "        return total_loss / self.num_samples\n",
    "    \n",
    "    def reset(self):\n",
    "        self.betha = self.init_params()\n",
    "        \n",
    "    def step_n(self, n=1000, lr=1e-2, visualize=False):\n",
    "        if visualize:\n",
    "            visualizer = RegressionVisualizer(self)\n",
    "            return visualizer.visualize_run(n=n, lr=lr)\n",
    "        \n",
    "        # Else, without visualization\n",
    "        for step_id in range(n):\n",
    "            parameters, loss = self.step(lr=lr)\n",
    "            \n",
    "        return parameters, loss\n",
    "\n",
    "        \n",
    "class RegressionVisualizer():\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        \n",
    "    def visualize_run(self, n=1000, lr=1e-2):   \n",
    "        fig, axs = self.init_axis(self.model.d if self.model.input_dim > 1 else self.model.d+1)\n",
    "        \n",
    "        # Run n steps   \n",
    "        for step_id in range(n):\n",
    "            parameters, loss = self.model.step(lr)\n",
    "            \n",
    "            self.plot_parameters(axs, step_id, parameters)\n",
    "            \n",
    "            # Plot the regression line\n",
    "            if self.model.input_dim == 1:\n",
    "                self.plot_regereession_line(axs)\n",
    "\n",
    "            display.clear_output(wait=True)\n",
    "            display.display(pl.gcf()) \n",
    "            \n",
    "            if self.model.input_dim == 1:\n",
    "                self.remove_regression_lines(axs)\n",
    "        \n",
    "        return parameters, loss\n",
    "    \n",
    "    def init_axis(self, num_plots, row_height=5, col_width=5):\n",
    "        fig, axs = plt.subplots(self.model.output_dim, num_plots, figsize=(num_plots*col_width, self.model.output_dim*row_height))\n",
    "            \n",
    "        # Initialize figure titles\n",
    "        for i in range(self.model.output_dim):\n",
    "            for j in range(self.model.d):\n",
    "                axs[i][j].set_title(f'betha_{i}_{j}')\n",
    "        \n",
    "        return fig, axs\n",
    "    \n",
    "    def plot_parameters(self, axs, step_id, parameters):\n",
    "        for j in range(self.model.output_dim):\n",
    "            for k in range(self.model.d):\n",
    "                axs[j][k].plot(step_id, self.model.actual_betha[j][k], 'bo')\n",
    "                axs[j][k].plot(step_id, parameters[j][k], 'ro')\n",
    "    \n",
    "    def plot_regereession_line(self, axs):            \n",
    "        for i in range(self.model.output_dim):\n",
    "            axs[i][-1].scatter(self.model.X, model.y[:, i], c='b')\n",
    "            axs[i][-1].plot(self.model.X, model.betha[i, :] @ model.data.T, 'r', label='Regression Line')\n",
    "            \n",
    "    def remove_regression_lines(self, axs):\n",
    "        for i in range(self.model.output_dim):\n",
    "            if len(axs[i][-1].lines) > 0:  # Check if there are lines plotted\n",
    "                axs[i][-1].lines[-1].remove()  # Remove the last line     \n",
    "        \n",
    "        \n",
    "        \n",
    "data, labels = generate_linear_regression_data(n=1000, input_dim=1, output_dim=3, noise=30)    \n",
    "model = LinearRegression(-1*data, labels)\n",
    "params, loss = model.step_n(n=50, lr=1e-1, visualize=True)\n",
    "print(params, loss)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
